{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm9JjFT1xvEx"
   },
   "source": [
    "Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install alibi-detect[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4AtL7MJVVNV"
   },
   "outputs": [],
   "source": [
    "from alibi_detect.od import OutlierAE, OutlierVAE\n",
    "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image\n",
    "import os\n",
    "from turtle import end_fill\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Reshape, InputLayer, Flatten\n",
    "from alibi_detect.od import OutlierAE, OutlierVAE\n",
    "from alibi_detect.utils.visualize import plot_instance_score, plot_feature_outlier_image\n",
    "import random\n",
    "import time\n",
    "import datetime as dt\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHt5_PS67LzH"
   },
   "outputs": [],
   "source": [
    "def trainimgload(directory,crop_size,size,testn):\n",
    "    print('load 시작')\n",
    "    dataset = []  #Many ways to handle data, you can use pandas. Here, we are using a list format.\n",
    "    images = os.listdir(directory)\n",
    "    images.sort()\n",
    "    for i, image_name in enumerate(images):\n",
    "        if (image_name.split('.')[-1] == 'png'):\n",
    "            image = cv2.imread(directory+ image_name)\n",
    "            image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(image, 'RGB')\n",
    "            if crop_size==0:\n",
    "                pass\n",
    "            else:\n",
    "                image = image.crop(crop_size)\n",
    "            image = image.resize((size, size))\n",
    "            dataset.append(np.array(image))\n",
    "    dataset = np.array(dataset)\n",
    "    n=np.shape(dataset)[0]\n",
    "    #trainn=round(n*trainratioP)\n",
    "    trainn=n-testn\n",
    "    if testn!=0:\n",
    "        train = dataset[0:trainn]\n",
    "        test = dataset[trainn:]\n",
    "    else:\n",
    "        train=dataset[0:n-1]\n",
    "        testn=1\n",
    "        test=dataset[n-1:]\n",
    "    trainn=np.shape(train)[0]\n",
    "    testn=np.shape(test)[0]\n",
    "    train = train.astype('float32') / 255.\n",
    "    test = test.astype('float32') / 255.\n",
    "    print(\"train 갯수: \", trainn)\n",
    "    print(\"val 갯수: \", testn)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8_EwnsTcNtH"
   },
   "source": [
    "이미지 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59TU8N7n6t3c"
   },
   "outputs": [],
   "source": [
    "def imgload(directoryP,crop_size,SIZE,gray):\n",
    "    image_directory = directoryP\n",
    "    dirname=directoryP.split('/')[-2]\n",
    "    #print(dirname, 'load시작')\n",
    "    images = os.listdir(image_directory)\n",
    "    images.sort()\n",
    "    print(images)\n",
    "    imgdataset=[]\n",
    "    for i, image_name in enumerate(images):\n",
    "        if (image_name.split('.')[-1] == 'png'):\n",
    "            image = cv2.imread(image_directory + image_name)\n",
    "            if gray==1:\n",
    "                image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "                image = Image.fromarray(image,mode=\"L\")\n",
    "            else:\n",
    "                image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "                image = Image.fromarray(image, 'RGB')\n",
    "            if crop_size==0:\n",
    "                pass\n",
    "            else:\n",
    "                image = image.crop(crop_size)\n",
    "            image = image.resize((SIZE, SIZE))\n",
    "            if gray==1:\n",
    "                image = np.reshape(image,(SIZE,SIZE,1))\n",
    "            # image=np.array(image)\n",
    "            # image[0:16,48:64,:]=0\n",
    "            imgdataset.append(np.array(image))\n",
    "    imgdataset = np.array(imgdataset)\n",
    "    imgdatasetn=np.shape(imgdataset)\n",
    "    imgdataset = imgdataset.astype('float32') / 255.\n",
    "    # print(images[0])\n",
    "    if imgdatasetn[0]!=1:\n",
    "        print(dirname, \" data number: \", imgdatasetn[0])\n",
    "    return imgdataset, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iPoBlU71W4k"
   },
   "outputs": [],
   "source": [
    "crop_size=0\n",
    "dir=\"#####빈칸을 채우세요######\"\n",
    "train,val=trainimgload(dir,crop_size,64,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZrYIxcK61X2"
   },
   "outputs": [],
   "source": [
    "crop_size=0\n",
    "dir=\"#####빈칸을 채우세요######\"\n",
    "test,name=imgload(dir,crop_size,64,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fvnUXrShC2w"
   },
   "outputs": [],
   "source": [
    "plt.imshow(test[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJsFwm2rcHZt"
   },
   "source": [
    "모델 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO5qJmXv5lnT"
   },
   "outputs": [],
   "source": [
    "size=64\n",
    "#모델 변수\n",
    "encoding_dim =1024\n",
    "latent_dim = 1024\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "train_thresehold=99.0\n",
    "ep=50\n",
    "batsize=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpH1WoZtcFbM"
   },
   "source": [
    "모델 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwBzHp3jMW7o"
   },
   "outputs": [],
   "source": [
    "dense_dim = [8, 8, 512] #Dimension of the last conv. output. This is used to work our way back in the decoder.\n",
    "#Define encoder\n",
    "print(train[0].shape)\n",
    "encoder_net = tf.keras.Sequential(\n",
    "    [\n",
    "        InputLayer(input_shape=train[0].shape),\n",
    "        Conv2D(##, ##, strides=##, padding='same', activation=tf.nn.relu),\n",
    "        Conv2D(###, ##, strides=###, padding='same', activation=tf.nn.relu),\n",
    "        Conv2D(###, #, strides=###, padding='same', activation=tf.nn.relu),\n",
    "        Flatten(),\n",
    "        Dense(encoding_dim,)\n",
    "    ])\n",
    "print(encoder_net.summary())\n",
    "\n",
    "decoder_net = tf.keras.Sequential(\n",
    "    [\n",
    "        InputLayer(input_shape=(encoding_dim,)),\n",
    "        Dense(np.prod(dense_dim)),\n",
    "        Reshape(target_shape=dense_dim),\n",
    "        Conv2DTranspose(###, ##, strides=##, padding='same', activation=tf.nn.relu),\n",
    "        Conv2DTranspose(###, ##, strides=##, padding='same', activation=tf.nn.relu),\n",
    "        Conv2DTranspose(###, ##, strides=##, padding='same', activation='sigmoid')\n",
    "    ])\n",
    "print(decoder_net.summary())\n",
    "\n",
    "#latent_dim = latent_dim\n",
    "# initialize outlier detector\n",
    "od = OutlierVAE(threshold=.015,  # threshold for outlier score above which the element is flagged as an outlier.\n",
    "                score_type='mse',  # use MSE of reconstruction error for outlier detection\n",
    "                encoder_net=encoder_net,  # can also pass VAE model instead\n",
    "                decoder_net=decoder_net,  # of separate encoder and decoder\n",
    "                latent_dim=latent_dim,\n",
    "                samples=20)\n",
    "print(\"Current threshold value is: \", od.threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqES7Md1cDy0"
   },
   "source": [
    "학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c9lZTj3MYhT"
   },
   "outputs": [],
   "source": [
    "od.fit(train,\n",
    "       optimizer = adam,\n",
    "       epochs=ep,\n",
    "       batch_size=batsize,\n",
    "       verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8afiWjWLcAzw"
   },
   "source": [
    "Threshold 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0I2FpI1FCgX"
   },
   "outputs": [],
   "source": [
    "od.infer_threshold(val, outlier_type='instance', threshold_perc=train_thresehold)\n",
    "print(\"Current threshold value is: \", od.threshold)\n",
    "\n",
    "X = val\n",
    "od_preds = od.predict(X,\n",
    "                      outlier_type='instance',    # use 'feature' or 'instance' level\n",
    "                      return_feature_score=True,  # scores used to determine outliers\n",
    "                      return_instance_score=True)\n",
    "#Scatter plot of instance scores. using the built-in function for the scatterplot.\n",
    "target = np.zeros(X.shape[0],).astype(int)  # Ground truth (all ones for bad images)\n",
    "labels = ['normal', 'outlier']\n",
    "plot_instance_score(od_preds, target, labels, od.threshold) #pred, target, labels, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHyvXzBCb5bB"
   },
   "source": [
    "Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ti3upBLhHG6A"
   },
   "outputs": [],
   "source": [
    "from alibi_detect.utils.saving import save_detector\n",
    "filepath=\"#####빈칸을 채우세요######\"\n",
    "save_detector(od, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZ9q191Ub3JN"
   },
   "source": [
    "Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwxc5RV1NvW6"
   },
   "outputs": [],
   "source": [
    "filepath=\"#####빈칸을 채우세요######\"\n",
    "from alibi_detect.utils.saving import load_detector\n",
    "od = load_detector(filepath)\n",
    "print(\"Current threshold value is: \", od.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm49Q_tCM-Lp"
   },
   "outputs": [],
   "source": [
    "a=0\n",
    "X = test[5*a:5*a+5]\n",
    "od_preds = od.predict(X,\n",
    "                      outlier_type='instance',    # use 'feature' or 'instance' level\n",
    "                      return_feature_score=True,  # scores used to determine outliers\n",
    "                      return_instance_score=True)\n",
    "X_recon = od.vae(X).numpy()\n",
    "plot_feature_outlier_image(od_preds,\n",
    "                           X,\n",
    "                           X_recon=X_recon,\n",
    "                           max_instances=5,  # max nb of instances to display\n",
    "                           outliers_only=False,\n",
    "                           n_channels=3)  # only show outlier predictions\n",
    "ascore = od_preds['data']['instance_score']\n",
    "print(name[5*a:5*a+5])\n",
    "print(ascore)\n",
    "target = np.zeros(X.shape[0],).astype(int)  # Ground truth (all ones for bad images)\n",
    "labels = ['normal', 'outlier']\n",
    "plot_instance_score(od_preds, target, labels, od.threshold)\n",
    "print(\"Current threshold value is: \", od.threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihZkakQ6o3tL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPBbsAW4KkowXOf287qKmGg",
   "gpuType": "T4",
   "mount_file_id": "1Pvlb3wYx31QSwg-Phkr7zxAqNos9IYeT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
